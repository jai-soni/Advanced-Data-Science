{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Energy Consumption Prediction By Appliances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Required Libraries\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.cross_validation import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trn = pd.read_csv('training.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('testing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weekstatus(x):\n",
    "    return 0 if x == 'Weekday' else 1\n",
    "\n",
    "def featureengineering(df):\n",
    "    # Converting datatype of Date column to date time\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    # Converting values of Day of week as Monday:0 , Tuesday:1 ...\n",
    "    df['Day_of_week'] = df['date'].dt.weekday\n",
    "    # Converting values of weekstatus as Weekday:0 and Weekend:1\n",
    "    df['WeekStatus'] = df['WeekStatus'].apply(lambda x:weekstatus(x))\n",
    "    # Assigning Features and Target to X and Y\n",
    "    X = df.drop(['date','Appliances','rv1','rv2','WeekStatus', 'Day_of_week','T6', 'T9'],axis =1)\n",
    "    Y = df['Appliances']\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datapreprocessing(df_trn,df_test):\n",
    "    \n",
    "    # Remove correlated features T6 and T9\n",
    "    train = df_trn.drop(['date','rv1','rv2','WeekStatus', 'Day_of_week','T6', 'T9'], axis=1)\n",
    "    test = df_test.drop(['date','rv1','rv2','WeekStatus', 'Day_of_week','T6', 'T9'], axis=1)\n",
    "    \n",
    "    # Scales the data to zero mean and unit variance\n",
    "    standard_scaler = StandardScaler()\n",
    "    \n",
    "    # Create dummy dataframes to hold the scaled train and test data\n",
    "    train_scaled = pd.DataFrame(columns=train.columns, index=train.index)\n",
    "    test_scaled = pd.DataFrame(columns=test.columns, index=test.index)\n",
    "\n",
    "    # Store the scaled data in new dataframes\n",
    "    train_scaled[train_scaled.columns] = standard_scaler.fit_transform(train)\n",
    "    test_scaled[test_scaled.columns] = standard_scaler.fit_transform(test)\n",
    "    \n",
    "    # Prepare training and testing data\n",
    "    X_trn = train_scaled.drop(\"Appliances\", axis=1)\n",
    "    y_trn = train_scaled[\"Appliances\"]\n",
    "\n",
    "    X_test = test_scaled.drop(\"Appliances\", axis=1)\n",
    "    y_test = test_scaled[\"Appliances\"]\n",
    "    \n",
    "    return X_trn, y_trn, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled_trn, y_scaled_trn, X_scaled_test, y_scaled_test = datapreprocessing(df_trn,df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_Implementation(X_trn,y_trn,X_test,y_test):\n",
    "    \n",
    "    models = [ExtraTreesRegressor(random_state=42)]\n",
    "\n",
    "    TestModels = pd.DataFrame()\n",
    "    tmp = {}\n",
    "\n",
    "    for model in models:\n",
    "        # get model name\n",
    "        m = str(model)\n",
    "        tmp['Model'] = m[:m.index('(')]\n",
    "        # fit model on training dataset\n",
    "\n",
    "        start = time()\n",
    "        model.fit(X_trn, y_trn)\n",
    "        end = time()\n",
    "\n",
    "        #Predictions and Validation for Testing and Training Set\n",
    "        predictions = model.predict(X_test)\n",
    "        predictions_trn = model.predict(X_trn)\n",
    "        #R2 score\n",
    "        tmp['R2_Test'] = round(r2_score(y_test,predictions),3)\n",
    "        tmp['R2_Train'] = round(r2_score(y_trn,predictions_trn),3)\n",
    "        #Mean Absolute Error(MAE)\n",
    "        tmp['MAE_Test']= round(mean_absolute_error(y_test,predictions),3)\n",
    "        tmp['MAE_Train']= round(mean_absolute_error(y_trn,predictions_trn),3)\n",
    "        #Mean Squared Error(MSE)\n",
    "        tmp['MSE_Test']= round(mean_squared_error(y_test,predictions),3)\n",
    "        tmp['MSE_Train']= round(mean_squared_error(y_trn,predictions_trn),3)\n",
    "        #Root Mean Squared Error (RMSE)\n",
    "        tmp['RMSE_Test'] = round(np.sqrt(mean_squared_error(y_test,predictions)),3)\n",
    "        tmp['RMSE_Train'] = round(np.sqrt(mean_squared_error(y_trn,predictions_trn)),3)\n",
    "        #Mean Absolute Percentage Error\n",
    "        tmp['MAPE_Test'] =  round(np.mean(np.abs((y_test - predictions) / y_test)) * 100,3)\n",
    "        tmp['MAPE_Train'] =  round(np.mean(np.abs((y_trn - predictions_trn) / y_trn)) * 100,3)\n",
    "        #Training and Testing Scores\n",
    "        tmp['Training Score(%)'] = round(model.score(X_trn, y_trn) * 100,3)\n",
    "        tmp['Testing Score(%)'] = round(model.score(X_test, y_test) * 100,3)\n",
    "        # Training Time\n",
    "        tmp['Training Time'] = round(end-start,3)\n",
    "        # write obtained data\n",
    "        TestModels = TestModels.append([tmp])\n",
    "\n",
    "    TestModels.set_index('Model', inplace=True)\n",
    "    #print('Iteration'+str(i))\n",
    "    TestModels\n",
    "    return TestModels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAE_Test</th>\n",
       "      <th>MAE_Train</th>\n",
       "      <th>MAPE_Test</th>\n",
       "      <th>MAPE_Train</th>\n",
       "      <th>MSE_Test</th>\n",
       "      <th>MSE_Train</th>\n",
       "      <th>R2_Test</th>\n",
       "      <th>R2_Train</th>\n",
       "      <th>RMSE_Test</th>\n",
       "      <th>RMSE_Train</th>\n",
       "      <th>Testing Score(%)</th>\n",
       "      <th>Training Score(%)</th>\n",
       "      <th>Training Time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ExtraTreesRegressor</th>\n",
       "      <td>0.307</td>\n",
       "      <td>0.0</td>\n",
       "      <td>124.967</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.451</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.549</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.671</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.934</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.426</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     MAE_Test  MAE_Train  MAPE_Test  MAPE_Train  MSE_Test  \\\n",
       "Model                                                                       \n",
       "ExtraTreesRegressor     0.307        0.0    124.967         0.0     0.451   \n",
       "\n",
       "                     MSE_Train  R2_Test  R2_Train  RMSE_Test  RMSE_Train  \\\n",
       "Model                                                                      \n",
       "ExtraTreesRegressor        0.0    0.549       1.0      0.671         0.0   \n",
       "\n",
       "                     Testing Score(%)  Training Score(%)  Training Time  \n",
       "Model                                                                    \n",
       "ExtraTreesRegressor            54.934              100.0          1.426  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_scaled = model_Implementation(X_scaled_trn, y_scaled_trn, X_scaled_test, y_scaled_test)\n",
    "results_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Hyperparameter Tuning</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Hyper Parameters selection for Extra Trees Regressor</h4>\n",
    "\n",
    "We will try adjusting the following set of hyperparameters:\n",
    "\n",
    "- n_estimators = number of trees in the foreset\n",
    "- max_features = max number of features considered for splitting a node\n",
    "- max_depth = max number of levels in each decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 - Randomized Search\n",
    "\n",
    "The most important arguments in RandomizedSearchCV are n_iter, which controls the number of different combinations to try, and cv which is the number of folds to use for cross validation (we use 20 and 5 respectively). More iterations will cover a wider search space and more cv folds reduces the chances of overfitting, but raising each will increase the run time. Machine learning is a field of trade-offs, and performance vs time is one of the most fundamental."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  4.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Initialize the model based on best performance from above, We got ExtraTreesRegressor \n",
    "sel_model = ExtraTreesRegressor(random_state=42)\n",
    "\n",
    "# Define the parameter subset\n",
    "\n",
    "param_grid = {\n",
    "    \"n_estimators\": [10, 50, 100, 200, 250, 300, 500, 800],\n",
    "    \"max_features\": [\"auto\", \"sqrt\", \"log2\"],\n",
    "    \"max_depth\": [None, 10, 50, 100, 200, 500]\n",
    "}\n",
    "\n",
    "# Use Randomized search to try 20 subsets from parameter space with 5-fold cross validation\n",
    "random_search = RandomizedSearchCV(sel_model, param_grid, n_iter=20, scoring=\"r2\", cv=5, n_jobs=-1, verbose=2, random_state=42)\n",
    "random_search.fit(X_scaled_trn, y_scaled_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 200, 'max_features': 'log2', 'max_depth': None}\n"
     ]
    }
   ],
   "source": [
    "# Best Parameters for the model from Randomized Search CV\n",
    "print(random_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to measure performance\n",
    "def evaluate(model, test_features, test_labels):\n",
    "    predictions = model.predict(test_features)\n",
    "    errors = abs(predictions - test_labels)\n",
    "    mape = 100 * np.mean(errors / test_labels)\n",
    "    rmse = np.sqrt(mean_squared_error(test_labels,predictions))\n",
    "    r2 = model.score(test_features, test_labels)\n",
    "    print('Model Performance')\n",
    "    print('R2 Test: {:0.3f}'.format(r2))\n",
    "    print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n",
    "    print('RMSE Test: {:0.4f}'.format(rmse))    \n",
    "    return r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BaseEstimator.get_params of ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,\n",
       "          max_features='auto', max_leaf_nodes=None,\n",
       "          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "          min_samples_leaf=1, min_samples_split=2,\n",
       "          min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "          oob_score=False, random_state=42, verbose=0, warm_start=False)>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel_model.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance\n",
      "R2 Test: 0.549\n",
      "Average Error: 0.3067 degrees.\n",
      "RMSE Test: 0.6713\n"
     ]
    }
   ],
   "source": [
    "base_model = ExtraTreesRegressor(random_state = 42)\n",
    "base_model.fit(X_scaled_trn, y_scaled_trn)\n",
    "base_accuracy = evaluate(base_model, X_scaled_test, y_scaled_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance\n",
      "R2 Test: 0.598\n",
      "Average Error: 0.2847 degrees.\n",
      "RMSE Test: 0.6341\n",
      "Wall time: 798 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "best_model = random_search.best_estimator_\n",
    "best_accuracy = evaluate(best_model, X_scaled_test, y_scaled_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improvement of 8.84%.\n"
     ]
    }
   ],
   "source": [
    "print('Improvement of {:0.2f}%.'.format( 100 * (best_accuracy - base_accuracy) / base_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random search allowed us to narrow down the range for each hyperparameter. Now that we know where to concentrate our search, we can explicitly specify every combination of settings to try. We do this with GridSearchCV, a method that, instead of sampling randomly from a distribution, evaluates all combinations we define. To use Grid Search, we make another grid based on the best values provided by random search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  37 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=2)]: Done  60 out of  60 | elapsed:  2.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'max_depth': 5, 'max_features': 'log2', 'n_estimators': 150}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    \"n_estimators\": [150,200,250,300,350],\n",
    "    \"max_features\": [\"log2\"],\n",
    "    \"max_depth\": [None,5,10,15]\n",
    "}\n",
    "\n",
    "# Create a based model\n",
    "sel_model = ExtraTreesRegressor(random_state=42)\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = sel_model, param_grid = param_grid, \n",
    "                          cv = 3, n_jobs = 2, verbose = 2)\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_scaled_trn, y_scaled_trn)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance\n",
      "R2 Test: 0.107\n",
      "Average Error: 0.5220 degrees.\n",
      "RMSE Test: 0.9450\n"
     ]
    }
   ],
   "source": [
    "best_grid = grid_search.best_estimator_\n",
    "grid_accuracy = evaluate(best_grid, X_scaled_test, y_scaled_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improvement of -80.51%.\n"
     ]
    }
   ],
   "source": [
    "print('Improvement of {:0.2f}%.'.format( 100 * (grid_accuracy - base_accuracy) / base_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "learning_curve() missing 3 required positional arguments: 'estimator', 'X', and 'y'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m----------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-4dfe47db0334>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlearning_curve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: learning_curve() missing 3 required positional arguments: 'estimator', 'X', and 'y'"
     ]
    }
   ],
   "source": [
    "learning_curve()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
